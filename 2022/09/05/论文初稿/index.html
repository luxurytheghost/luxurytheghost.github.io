<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"always","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="一. 数据采集为了的得到自由视点视频，首当其冲便是摄像机采集到合适的数据。数据采集的优化有两种方案，我们可以对单个相机进行优化，使其拥有更好的采集效果；也可以搭建多相机阵列，捕获更大视角的光场。下面我们对两种方案分别进行讨论。">
<meta property="og:type" content="article">
<meta property="og:title" content="初稿ver.01">
<meta property="og:url" content="http://example.com/2022/09/05/%E8%AE%BA%E6%96%87%E5%88%9D%E7%A8%BF/index.html">
<meta property="og:site_name" content="luxurythrghost&#39;s blogs">
<meta property="og:description" content="一. 数据采集为了的得到自由视点视频，首当其冲便是摄像机采集到合适的数据。数据采集的优化有两种方案，我们可以对单个相机进行优化，使其拥有更好的采集效果；也可以搭建多相机阵列，捕获更大视角的光场。下面我们对两种方案分别进行讨论。">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2022-09-05T12:00:00.000Z">
<meta property="article:modified_time" content="2022-09-07T14:12:39.842Z">
<meta property="article:author" content="luxutytheghost">
<meta property="article:tag" content="-分享">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/2022/09/05/%E8%AE%BA%E6%96%87%E5%88%9D%E7%A8%BF/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>初稿ver.01 | luxurythrghost's blogs</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">luxurythrghost's blogs</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-schedule">

    <a href="/schedule/" rel="section"><i class="fa fa-calendar fa-fw"></i>日程表</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/09/05/%E8%AE%BA%E6%96%87%E5%88%9D%E7%A8%BF/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/aqua.jpg">
      <meta itemprop="name" content="luxutytheghost">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="luxurythrghost's blogs">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          初稿ver.01
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-09-05 20:00:00" itemprop="dateCreated datePublished" datetime="2022-09-05T20:00:00+08:00">2022-09-05</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-09-07 22:12:39" itemprop="dateModified" datetime="2022-09-07T22:12:39+08:00">2022-09-07</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%96%87%E7%AB%A0/" itemprop="url" rel="index"><span itemprop="name">文章</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="一-数据采集"><a href="#一-数据采集" class="headerlink" title="一. 数据采集"></a>一. 数据采集</h1><p>为了的得到自由视点视频，首当其冲便是摄像机采集到合适的数据。数据采集的优化有两种方案，我们可以对<strong>单个相机进行优化</strong>，使其拥有更好的采集效果；也可以搭建<strong>多相机阵列</strong>，捕获更大视角的光场。下面我们对两种方案分别进行讨论。</p>
<span id="more"></span>

<h3 id="单相机优化"><a href="#单相机优化" class="headerlink" title="单相机优化"></a>单相机优化</h3><p>传统相机一般不会完全记录整个4D光场的分布，这将丢失大部分从世界进入的光在光场中的分布信息。文献[1]作者提出方案，通过在传感器和主透镜之间插入一个微透镜阵列，提出了一种可在单次摄影曝光中对其传感器上的4D光场进行采样的全光相机，实现了可以计算出聚焦在不同深度的清晰照片，更短的曝光和更低的图像噪点。而为了得到恢复的光场，文献[2]作者提出基于多频相移技术确定微透镜中心位置，通过对Lytro光场相机的实际标定，确定了原始数据上像元与空间中标定板上物点的对应关系以及像素与微透镜的所属关系。这一方案不仅恢复了光场信息，并且解决了白图像获取过程中漫反射光的不一致性问题。</p>
<p>此外，也有人通过掩膜来重建光场，并且对图像进行优化。方案一[3]提出了更容易调节的新型混合成像&#x2F;光场相机设计，在基于镜头的相机的光路中<strong>更换单个衰减掩模</strong>而不是透镜阵列来可逆地调制4D光场，实现了从2D相机图像重建4D光场，而无需像以前的光场相机所需的任何额外的折射元件，还可以恢复场景中对焦部分的全分辨率图像信息。另一种方案[4]则是在相机镜头与传感器之间<strong>添加一层随机的mask掩膜</strong>，镜头的对目标光场进行采集，计算机采用压缩感知对采集到的单张传感器编码图像进行非线性优化，恢复原始光场。</p>
<p>方案一中通过更换衰减掩膜将空间分辨率换成角度分辨率来处理光线，这将导致光场分辨率低，针对此问题有作者提出了[5]<strong>采用最佳多路复用方案和使用图案滚动和液晶阵列设计可编程孔径的两个新原型</strong>，可编程光圈的形状可以调整并用于通过多次曝光以全传感器分辨率捕获光场，无需任何额外的光学元件，也无需移动相机，同时提出了校准算法和深度估计算法，实现了高采集效率，提高了光场的角分辨率。</p>
<h3 id="多相机阵列"><a href="#多相机阵列" class="headerlink" title="多相机阵列"></a>多相机阵列</h3><p>多数都使用单个移动的高质量相机来查看静态场景，而自由视点视频一般需要构成动态场景，为了保证在动态场景中的图像质量，需要多个摄像头。因此有人尝试使用100个自定义摄像机构成阵列[6]，在一个或多个轴（例如分辨率、动态范围、帧速率和&#x2F;或大光圈）上逼近具有高性能的传统单中心投影摄像机，使用多个摄像机逼近视频具有大合成光圈的相机，实现了捕获视频光场，可以对其应用时空视图插值算法，以便以数字方式模拟时间膨胀和相机运动，还可以使用自定义非均匀合成孔径创建视频序列。但是，摄像机太多将导致音画不同步的问题，而语音和图像也不能进行单独处理，有作者[7]提出了一个<strong>多维多点测量系统</strong>，以 <em>同步</em> 方式采集100多个点的视频和声音，使用GPS传感器在远程站点同步，实现了视频与模拟信号的高精度同步。这些方法虽然效果较好，但是占用资源较高，因此有作者提出一种[8]<strong>基于压缩感知的稀疏相机阵列光场采集与恢复方法</strong>，以1x5的稀疏相机阵列恢复成1x24的稠密水平光场,增加了光的通透率，并且明显的降低了硬件成本。</p>
<p>但是，即使是稀疏相机阵列，依旧存在一个无法忽视的问题，那就是高精度的视频产生的带宽将极高，一般的ip网络很难在观看流媒体视频的同时在场景中交互式无缝漫游。针对此问题文献[9]作者提出了由一个 8×8 光场相机阵列、16 台生产者 PC、一个流媒体服务器系统和几个客户端组成的系统，实现了一个具有实时数据采集、压缩、互联网传输、光场渲染和动态场景自由视点控制的 3D 电视系统，多个视频流以实时方式编码，每个客户端可以自由选择流进行新颖的视图渲染，大大降低了带宽的需求。</p>
<h1 id="二-数据处理"><a href="#二-数据处理" class="headerlink" title="二. 数据处理"></a>二. 数据处理</h1><p>在摄像机采集到数据之后，由于摄像机的参数不同，拍摄时角度不同等各种因素，我们还要对摄像机采集到的数据进行处理。大体上分为<strong>颜色校正，深度图构建，超分辨率重建</strong>三个部分。</p>
<h3 id="颜色校正"><a href="#颜色校正" class="headerlink" title="颜色校正"></a>颜色校正</h3><p>不同参数的摄像机的色域可能不同，相对比较早的方案[10]是使用<strong>尺度不变特征变换（SIFT）</strong>检测对应关系，独立处理RGB通道，使用能量最小化方法计算查找表，并使用这些表校正捕获的视频。 另一种使用SIFT的方案[11]则是通过SIFT算法提取参考图像和待校正图像的特征点；然后根据匹配算法得到对应的匹配点；再通过匹配点的特征得到R、G、B分量的校正因子，并校正得到最终序列。校正在RGB颜色空间中完成，需要进行 YUV和RGB之间的转换，该方法对纹理细节较多的序列效果更明显。</p>
<p>多视点视频使用了大量的摄像机，故视点间颜色可能不一致，这将为后续的视频编码和虚拟视点绘制带来困难。因此有作者提出一种面向编码和绘制的多视点视频颜色校正方法[12]，首先采用连续多帧求帧差法提取出各个视点图像的背景区域，再利用背景区域计算校正因子对颜色不一致的视点进行颜色校正，以此来解决视点间颜色不一致的问题。这个方法是在时间上进行处理，基于空间方面，有作者提出了一种基于尺寸可变块匹配的多视点颜色校正方法[13]。该方法首先通过视差估计得到匹配块；然后根据残差能量对其可靠性进行分类。不可靠块采用四叉树分割和可变块视差估计进行重新搜索。逐步消除由于匹配块不可靠性带来的方块效应和边缘模糊现象；最后通过线形回归方法得到颜色校正系数。并且采用亮度直方图均衡进一步提高了校正质量。</p>
<h3 id="深度图构建"><a href="#深度图构建" class="headerlink" title="深度图构建"></a>深度图构建</h3><p>深度图可以通过各种设备获取，包括飞行时间、Kinect或光场相机。而深度图一般会有伪影，我们可以从多个方面减少伪影。例如在视图，空间，时间等方面均可对伪影进行处理。但是一般在三维视频系统的非对称框架中，为了构建动态的三维场景，需要进行密集深度的图像合成。因此文献作者提出了一种通过能量最小化的三维视频不对称框架的合成方法[14]。在提出的能量函数中处理了基于块的标签和度量函数，并提出了一种多标签图结构来最小化能量函数。此方法应用于三维视频编码的非对称方案后，可以获得某些测试序列的BD-PSNR或BD-BR增益。根据结果，当计算资源有限时，基于块的密集深度合成效果更好。</p>
<h3 id="超分辨率重建"><a href="#超分辨率重建" class="headerlink" title="超分辨率重建"></a>超分辨率重建</h3><p>为了得到FHD,QHD,乃至UHD等级的视频，我们需要对视频图像进行超分辨率重建。将超分辨率图像重建问题按照不同的输入输出情况进行系统分类, 将超分辨率问题分为基于深度图重建的超辨率、视频超分辨率、单帧图像超分辨率三大类。前面两种比较常用，而卷积神经网络又大大提高了重建的效率。针对深度图重建，文献作者出一种基于卷积神经网络的深度图超分辨率重建算法[15]。该算法不需要提前对特定的任务从图像中提取具体的手工特征，而是模拟人类的视觉系统对原始深度图进行层次化的抽象处理以自主地提取特征。该算法直接进行从低分辨率深度图到高分辨率深度图的映射学习。映射由７个卷积层和１个反卷积层联合实现。卷积操作学习丰富的图像特征，而反卷积实现上采样重建高分辨率的深度图。该方案解决了传统深度图超分辨率重建算法需要人工提取特征、计算复杂度较高且不容易得到合适表示特征的问题。而针对视频超分辨率处理，也可以使用卷积神经网络来实现，有人提出了一种采用精简卷积神经网络的快速视频超分辨率重建方法[16]。首先 ，考虑到输入的尺寸大小会直接影响网络的运算速度，所提网络省去传统方法的预插值过程，直接对多个低分辨率输入视频帧提取特征，并进行多维特征通道融合。接着，为了避免网络中产生零梯度而丢失视频的重要信息，采用参数线性纠正单元作为激活函数，并采用尺寸更小的滤波器调整网络结构以进行多层映射。最后，在网络末端添加反卷积层上采样得到重建视频。该方案解决了传统重建方法忽略了视频帧之间的时域相关性的问题，并且重建速度更快。</p>
<h1 id="三-视频编解码"><a href="#三-视频编解码" class="headerlink" title="三. 视频编解码"></a>三. 视频编解码</h1><p>目前，应用在自由视点视频编码的主要技术是MV-HEVC(多视点高效视频编码)，该编码为HEVC标准的多视点扩展，该扩展无论论是否附带深度图，都可以进行多视点视频编码。该标准中的H.264,H.265最初用于2D视频， 它们的多视点扩展的一个基本原则是重用底层2D编码的编码工具，相对的，3D编码扩展则引入了新的底层编码工具，用于提高编码效率。3D编码可通过帧编码，深度图编码等方式来实现，下面我们分别进行讨论。</p>
<h3 id="帧内以及帧间编码"><a href="#帧内以及帧间编码" class="headerlink" title="帧内以及帧间编码"></a>帧内以及帧间编码</h3><p>自由视点视频的视角很多，摄像机拍摄的视角不同，难免会产生视差，我们需要对其进行运动视差。首先文献作者提出了一种基于视点间运动补偿算法的侧信息生成方法[17]， 用于多视点视频的联合重建，然后将其与接收到的测量结果结合，进行最终的视频解码恢复。但是运动视差估计将给外部存储器带来巨大的开销，为此有文献作者提出一种<strong>内容自适应的参考帧压缩方案</strong>[18]。该方案基于简化的内预测过程， 以减少参考样本的空间冗余。预测帧内残差由非线性量化和基于huffman的熵编码器组成的路径压缩。根据内容适应并作出动态选择，该策略根据空间相似度对视频原始区块进行分类。实验结果表明新方案可大幅减少外部存储器的访问量，提高效率。此外，还有作者针对此问题提出联播编码的方案[19]，每个视图都是用分层双预测结构进行单独编码，并通过相应视图的动态背景建模生成额外的虚拟参考帧。方案三[20]则是使用视差三维校正MVC和三维运动估计来提高编码效率。使用所有视差校正视图的相同时间帧作为3D帧，并直接使用前3D帧作为参考帧对当前3D宏块进行三维运动估计， 然后使用3D编码技术进行压缩。相比传统的H.264和mvc,提高了交互性，并缩短了计算时间。</p>
<p>标准的视频编码算法采用差分编码，利用相邻帧的时间相关性来获得编码增益，网络中丢失的帧将导致解码器后续帧的误差传播。以分布式源码帧可以实现定期输入， 通过克服编码器的不确定性停止这种错误传播。为降低其编码复杂度，一种方案[21]采用了一种统一的IMVSDSC帧结构，使编码器停止错误传播，并且促进周期性的视图交换。此外，设计了一种多视图帧预测结构，以便给定带宽约束下最大化解码器正确解码的帧数。而另一种方案怕[22]提出的编码方案通过全局差异来确定基础视点的位置，并且引入合理的视图预测结构，基于GOP的长度<br>与随机存取访问性能之间的关系来选择GOP的帧数。该方案在保持编码效率的同时，可以显著提高随机访问性能。</p>
<h3 id="深度图编码"><a href="#深度图编码" class="headerlink" title="深度图编码"></a>深度图编码</h3><p>多视点编码可能有可能在不降低编码性能的情况下避免摄像机之间的通信，然而，当结果可能与预期不匹配。有文献作者提出一种新的编码方案[23]，其中深度图用于估计，两个视图和摄像机之间没有相关性，只有相对位置已知。该文章设计了完整的方案，并进一步提出了速率分配算法，以便在方案的不同组件间共享位运算。</p>
<p>高效视频编码（3D-HEVC）标准的3D视频编码扩展可以有效的增加沉浸感，此扩展采用多视图视频加深度 （MVD） 格式，其中即时虚拟视图纹理视频可以通过相邻的深度图进行渲染，以便编码保存。多视图视频的运行时，深度图的良好保留对于合成高质量的虚拟视图纹理视频至关重要。 因此，在3D-HEVC中 引入了几种新的深度图编码工具，例如  深度建模模式（DMM）决策。 然而，众多DMM导致巨大的计算复杂性，这阻碍了3D-HEVC的实际应用。为此，文献作者提出了一种用于3D-HEVC深度图编码的高效DMM 决策算法[24]，该算法结合了不必要的深度块跳过方法和等值线模式。结果表明，所提出的高效DMM决策方法  平均可节省  约31.33%的编码时间。与原始的3D-HEVC编码器相比  ，比特率平均仅增加0.60%。此外，DMM中楔形分区模式复杂度比较高，有作者[25]对此提出在深度图的内部预测中确定楔形分区，并且以此来提高解码器端的计算效率和BD速率。</p>
<h3 id="编解码结构优化"><a href="#编解码结构优化" class="headerlink" title="编解码结构优化"></a>编解码结构优化</h3><p>已有的多视点视频编码标准，一般都是使用空间，时间相关性，来压缩来自于不同摄像机的，同一场景的图像。都是压缩的纹理和深度数据通常具有依赖关系，可能不适合可交互自由视点视频流系统 [interactivemultiview video streaming (IMVS) systems]。基于此情况，一种解决方案是在相关约束下选择预测结构，并基于纹理和深度参数， 使得视频失真最小化[26]。</p>
<p>除了增加编码端的功能以外，还可以在解码端构建具有非复杂导航功能的交互式多视点视频系统。构建多视点视频平台的工作十分具有挑战性。在有效利用存储和带宽资源，系统反应，体验质量，系统复杂性等方面都存在问题。用于生成虚拟视图的经典解码系统首先将参考或编码帧投影到给定的视图，然后由于潜在的遮挡而填充孔。但是最后一步仍然构成在接收器上使用特定软件或硬件的复杂操作，并且需要来自相邻帧的一定数量的信息，以确保虚拟映像之间的一致性。文章提出的方法[27]通过预测解码器的导航并发送保证时间一致性的辅助信息，将交互的负担从解码器转移到编码器。这会导致在传输速率和存储方面的额外成本，但是可使用基于用户行为建模的优化技术将其降至最低，有效降低编码端的压力。</p>
<h1 id="四-数据传输"><a href="#四-数据传输" class="headerlink" title="四. 数据传输"></a>四. 数据传输</h1><p>自由视点视频需要的沉浸式体验，要求网络具有低延迟、大带宽、高可靠性等特点。传输数据量高，需要网络的高稳定性，因此对网络提出了很高的要求，我们在传输速率，延迟降低，稳定网络等方面进行讨论。</p>
<h3 id="传输速率"><a href="#传输速率" class="headerlink" title="传输速率"></a>传输速率</h3><p>目前通讯技术飞速发展，第五代（5G）无线通信网络正在标准化，关键能力包括20Gbps的峰值数据率、0.1Gbps的用户体验数据率、毫秒级的端到端延迟，的区域流量容量等。与第四代（4G）无线通信系统相比。各种关键技术，如毫米波（mmWave）、大规模多输入多输出（MIMO）和超密集网络（UDN）等已经被提出来以实现5G的目标[28]。与5G相比，6G网络有望实现更高的性能。5G的峰值数据速率为20Gbps，而6G的网络在太赫兹和光频段的帮助下可以达到1-10 Tbps，区域流量容量可以超过 ，频谱效率可以提高3-5倍[29]，预计用户体验的数据速率为 1-10 Gbps，延迟约为10-100 µs[30]。</p>
<p>随着 5G和多接入边缘计算(MAEC)<br>技术的发展，云计算的思想已经成为实现移动端自由视点视频的重要途经。云计算是一种将资源和管理集中在云端的架构，它使终端设备和消费者能够进行弹性的按需资源分配，减少了管理工作量，以及方便了的应用和服务提供。文献展示了一种云服务的参考架构[31],对于客户来说，云计算可以让他们随时随地享受到高质量的服务体验，而无需购买昂贵的终端。</p>
<h3 id="降低延迟"><a href="#降低延迟" class="headerlink" title="降低延迟"></a>降低延迟</h3><p>为了提供极致的沉浸式体验，减少延迟是云计算系统最大的技术挑战之一。帧在云服务器上计算后，必须进行处理和编码，然后通过 5G网络传输。必需引入低延迟的视频压缩方案，以提高云服务的效率和客户体验。文献的作者提出了一种优化的编解码器来加速编码过程[32]。他们通过引入一些渲染信息（如相机位置、物体位置和相机与物体之间的距离等）来发这种方法。实验结果表明，在不降低图像质量的情况下，有可能节省 42％的编码时间。而另一位作者作者[33]通过引入运动估计探索了帧内插值的方法。他们使用图像单应性技术来进行更好的运动预测，并引入了一种专用的插值算法来进行精确的插值。实验结果表明，他们在整个编码过程上实现了最大18%的加速。</p>
<p>由于物理距离远、通信带宽有限、网络连接断断续续等原因，仅靠云计算可能无法满足自由视点视频的时延敏感的需求。为了在网络边缘提供云计算能力， 2014 年，欧洲电信标准协会(ETSI)内部的行业规范组提出了MEC[34]。正如ETSI所定义的，MEC是一个网络架构，在靠近移动用户的RAN内提供IT和云计算能力。</p>
<h3 id="稳定网络"><a href="#稳定网络" class="headerlink" title="稳定网络"></a>稳定网络</h3><p>自由视点视频应用过程中需要传输数据量大，受到网络带宽波动以及延时抖动等因素的干扰影响而导致流媒体数据传输存在质量不高的问题。RSVP 协议实现各个终端以及路由器之间的互动交流，将传输延时控制在一定区域范围当中[35]，确保端到端的通信服务质量安全无误。RTSP 协议初步满足了实时流媒体数据的多传输功能，且RTSP 协议在扩展性方面表现良好。</p>
<h1 id="五-视点合成"><a href="#五-视点合成" class="headerlink" title="五. 视点合成"></a>五. 视点合成</h1><p>目前常用的显示交互技术一般为DIBR(基于深度图像的渲染)，但是使用DIBR可能会带来严重的结构失真。此外，自由视点视频质量的提升也必须考虑在内。</p>
<h3 id="DIBR存在的问题"><a href="#DIBR存在的问题" class="headerlink" title="DIBR存在的问题"></a>DIBR存在的问题</h3><p>根据人类视觉系统(HVS)的结构，初级视觉皮层和大脑的其他部分的目标是减少输入视觉信号的冗余，以发现图像的内在结构，从而创造稀疏的图像表示。人类视觉系统（HVS）在感知视觉场景时，会在几个尺度和几个级别的分辨率上处理图像，因此HVS对于DIBR渲染产生的失真高度敏感。因此，我们可以从多个方面来降低DIBR的失真。</p>
<p>文献作者提出了一种新型的合成视频的无参考文献VQA方法[36]，该方法在空间和时间域中运行，被称为STD。在空间域，考虑到DIBR技术引入的几何失真会增加合成帧的高频内容，通过估计空间域中每个合成帧的高频能量，可以有效评估几何失真对合成视频视觉质量的影响。在时间域，通过测量连续帧之间的运动差异来量化时间上的不一致。具体来说，首先使用光流方法来估计相邻帧之间的运动场。然后，计算相邻光流场的结构相似度，并进一步采用结构相似度值对相邻光流场的像素差异进行加权。结果比最先进的I&#x2F;VQA方法有优势。</p>
<p>此外，在目前的DIBR系统中仍然存在一些麻烦的问题，如深度边缘错位、断层发生和重采样时的裂缝。有作者提出了一个强大的<strong>基于深度图像的立体视图合成</strong>的渲染方案[37]。该方案的核心是两个深度图滤波器，它们共享一个基于域变换的过滤框架。作为第一步，该框架的一个滤波器同时实现了纹理-深度边界的排列和方向性失配的减少平滑。然后，在对深度图进行三维翘曲后，在翘曲的深度图上使用另一个自适应滤波器，以传递场景梯度结构，进一步减少剩余的裂缝和噪音。最后，利用优化后的虚拟视图深度图，采用后向纹理翘曲来检索出最终的纹理虚拟视图。所提出的方案能够产生视觉上令人满意的结果，实现高质量的2D到3D转换。</p>
<h3 id="提高自由视点视频的质量"><a href="#提高自由视点视频的质量" class="headerlink" title="提高自由视点视频的质量"></a>提高自由视点视频的质量</h3><p>在自由视点视频中，可能会出现深度帧或者彩色帧丢失的情况。针对此情况，文献作者提出了两种方案，方案一是一种基于像素的颜色错误隐蔽方法[38]，并使用深度信息，该方法考虑了连续帧中的同一运动物体可能处于不同的深度的情况。在导出的运动矢量候选集中，考虑了所有的候选运动矢量，并通过深度差对参考像素进行加权，得到最终恢复的像素；方案二提出了一种迭代式深度帧误差掩蔽方法。最初恢复的深度帧是通过基于深度图像的渲染从另一个可用视图中获得的。然后按照提议的优先顺序填补恢复的深度帧中的漏洞。而深度帧和彩色帧均有丢失时，可考虑将两种方法一同使用，且恢复效果更好。</p>
<p>自由视点视频中一般会生成虚拟视点，为了提高自由视点视频系统的虚拟视图图像（VVI）的质量，文献提出了一种一种基于加权局部稀疏表示的深度图像超分辨率（WLDISR）方案[39]，与彩色图像不同，深度图像在合成VVI时主要用于提供几何信息。由于深度图像的纹理结构和平滑区域之间的视图合成特性不同，作者将深度图像分为边缘和平滑斑块，并分别学习两个局部字典。同时，在成本函数中明确加入权重项，以表示边缘结构和平滑区域对VVI质量的不同重要性。然后，局部稀疏表示和加权稀疏表示被共同用于深度图像超分辨率的字典学习和重建阶段。实验结果表明该方案在VVI上的质量方面获得了较高的平均收益。</p>
<p>通常自由视点视频会用于比赛等场景，在比赛等场景中，对人体姿势的捕捉尤为重要。由于复杂的深度网络架构和大量的三维人体姿势数据集，从单目中进行三维人体姿势估计已经显示出巨大的成功。然而，当这些数据集不可用时，它仍然是一个问题。有文献作者通过将相机视角信息与三维人体形状完全分离来克服投影模糊性问题[40]。具体来说，设计了一个因子化网络来预测两个独立通道中典型的三维人体姿势和摄像机视角的系数。在这里，将规范的三维人体姿势表示为来自字典的姿势基础的组合。为了保证因子化的一致性，设计了一个简单而有效的损失函数，利用了多视角信息。此外，为了从三维姿势系数中产生稳健的典型重建，利用人体姿势的基本三维几何学，从二维姿势中学习一个新的分层字典。与传统的单层字典相比，分层字典具有更强的三维姿势可表达性。实验结果表明，该法能够最大限度地分解三维人体形状和摄像机视角，并准确地重建三维人体姿势。此外，与最近的弱&#x2F;自监督方法相比，该方法取得了最先进的结果。</p>
<h1 id="文献参考"><a href="#文献参考" class="headerlink" title="文献参考"></a>文献参考</h1><ol>
<li>Ren N ,Levoy M ,Bredif M ;<br>Light Field Photography with a Hand-Held Plenopic Camera.  2005.</li>
<li>张旭,李晨; 微透镜阵列式光场成像模型及其标定方法.</li>
<li>Veeraraghavan A , Raskar R , Agrawal A ; Mask enhanced cameras for heterodyned light fields and coded aperture refocusing.</li>
<li>刘永春,龚华军,沈春林 基于掩膜的光场采集与重建的研究.</li>
<li>Chia-Kai, Liang, Tai-Hsu ; Programmable aperture photography: multiplexed light field acquisition.</li>
<li>Wilburn B ,  Joshi N ,  Vaish V;  High performance imaging using large camera arrays.</li>
<li>Fujii T ,  Mori K ,  Takeda K; Multipoint Measuring System for Video and Sound - 100-camera and microphone system.</li>
<li>刘永春,龚华军,沈春林 基于压缩感知的稀疏相机阵列光场采集与恢复.</li>
<li>Liu Y ,  Dai Q ,  Xu W; A Real Time Interactive Dynamic Light Field Transmission System.</li>
<li>Kenji Yamamoto∗  Ryutaro Oi; Color Correction for Multi-view Video Using Energy Minimization of View Networks.</li>
<li>孟念鹏 王 健 季晓勇; 一种基于SIFT的多视点视频颜色校正方法.</li>
<li>蒋刚毅 费跃 邵枫 彭宗举 郁梅; 面向编码和绘制的多视点图像颜色校正.</li>
<li>喻莉  熊玮  钟刚  邓慧萍;  基于尺寸可变块匹配的多视点视频颜色校正方法.</li>
<li>You Yang , Qiong Liu , Hao Liu , Li Yu , Fanglin Wang; Dense depth image synthesis via energy minimization for three-dimensional video.</li>
<li>李素梅，雷国庆，范如; 基于卷积神经网络的深度图超分辨率重建</li>
<li>潘志勇，郁 梅，谢登梅，宋 洋，蒋刚毅; 采用精简卷积神经网络的快速视频超分辨率重建.</li>
<li>Inter-view Motion Compensated Joint Decoding for Compressively-Sampled Multi-View Video Streams.</li>
<li>Content-adaptive reference frame compression based on intra-frame prediction for multiview video coding.  </li>
<li>Efficient Multi-view Video Coding using 3D Motion Estimation and Virtual Frame.</li>
<li>Disparity-adjusted 3D multi-view video coding with dynamic background modelling.</li>
<li>Unified Distributed Source Coding Frames for Interactive Multiview Video Streaming.</li>
<li>Multi- view Video Coding Scheme based upon enhanced Random Access capacity.</li>
<li>Depth-based multiview distributed video coding.</li>
<li>An efficient depth modeling mode decision algorithm for 3D-HEVC depth map coding.</li>
<li>A Fast Depth-Map Wedgelet Partitioning Scheme for Intra Prediction in 3D Video Coding .</li>
<li>Optimizing Multiview Video Plus Depth Prediction Structures for Interactive Multiview Video Streaming</li>
<li>Interactive multiview video system with non-complex navigation at the decoder.</li>
<li>Wang C X, Haider F, Gao X Q; Cellular architecture and key technologies for 5G wireless.</li>
<li>Chen S Z, Liang Y C, Sun S H. Vision, requirements, and technology trend of 6G: how to tackle thechallenges of system coverage, capacity, user data-rate and movement speed.</li>
<li>Zhang Z Q, Xiao Y, Ma Z, et al. 6G wireless networks: vision, requirements, architecture, and keytechnologies.</li>
<li>GSMA. Cloud AR&#x2F;VR white paper. 2019.</li>
<li>Liu Y, Dey S, Lu Y; Enhancing video encoding for cloud gaming using rendering information.</li>
<li>Xu L F, Guo X, Lu Y, et al. A low latency cloud gaming system using edge preserved image homography.</li>
<li>Chiang M, Zhang T; an overview of research opportunities. IEEE Internet Things J.</li>
<li>闻斌，庞璐宁，黄晟; 实时流媒体传输系统中关键技术的研究与实现.</li>
<li>Guangcheng Wang, Zhongyuan Wang, Ke Gu, Kui Jiang, Zheng He;  e Reference-Free DIBR-Synthesized Video Quality Metric in Spatial and Temporal Domains IEEE Transactions on Circuits and Systems for Video Technology.</li>
<li>LIU Wei, Yun Qi TANG, Jian Wei DING, Ming Yue CUI;<br>A Robust Depth Image Based Rendering Scheme for Stereoscopic View Synthesis with Adaptive Domain Transform Based Filtering Framework.</li>
<li>Ting-Lan Lin, Chuan-Jia Wang, Tsai-Ling Ding, Gui-Xiang Huang, Wei-Lin Tsai, Tsung-En Chang, Neng-Chieh Yang;  Recovery of Lost Color and Depth Frames in Multiview Videos IEEE Transactions on Image Processing.</li>
<li>Huan Zhang, Yun Zhang, Hanli Wang, Yo-Sung Ho, Shengzhong Feng ; WLDISR: Weighted Local SparseRepresentation-Based Depth Image Super-Resolution for 3D Video System IEEE Transactions on Image Processing.</li>
<li>Zhichao Ma, Kan Li, Yang Li ;  Self-supervised method for 3D human pose estimation with consistent shape andviewpoint factorization Applied Intelligence.</li>
</ol>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E5%88%86%E4%BA%AB/" rel="tag"># -分享</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2022/09/04/week-01/" rel="prev" title="week_01">
      <i class="fa fa-chevron-left"></i> week_01
    </a></div>
      <div class="post-nav-item"></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%B8%80-%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86"><span class="nav-number">1.</span> <span class="nav-text">一. 数据采集</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8D%95%E7%9B%B8%E6%9C%BA%E4%BC%98%E5%8C%96"><span class="nav-number">1.0.1.</span> <span class="nav-text">单相机优化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A4%9A%E7%9B%B8%E6%9C%BA%E9%98%B5%E5%88%97"><span class="nav-number">1.0.2.</span> <span class="nav-text">多相机阵列</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%BA%8C-%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86"><span class="nav-number">2.</span> <span class="nav-text">二. 数据处理</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%A2%9C%E8%89%B2%E6%A0%A1%E6%AD%A3"><span class="nav-number">2.0.1.</span> <span class="nav-text">颜色校正</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%B7%B1%E5%BA%A6%E5%9B%BE%E6%9E%84%E5%BB%BA"><span class="nav-number">2.0.2.</span> <span class="nav-text">深度图构建</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87%E9%87%8D%E5%BB%BA"><span class="nav-number">2.0.3.</span> <span class="nav-text">超分辨率重建</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%B8%89-%E8%A7%86%E9%A2%91%E7%BC%96%E8%A7%A3%E7%A0%81"><span class="nav-number">3.</span> <span class="nav-text">三. 视频编解码</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%B8%A7%E5%86%85%E4%BB%A5%E5%8F%8A%E5%B8%A7%E9%97%B4%E7%BC%96%E7%A0%81"><span class="nav-number">3.0.1.</span> <span class="nav-text">帧内以及帧间编码</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%B7%B1%E5%BA%A6%E5%9B%BE%E7%BC%96%E7%A0%81"><span class="nav-number">3.0.2.</span> <span class="nav-text">深度图编码</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BC%96%E8%A7%A3%E7%A0%81%E7%BB%93%E6%9E%84%E4%BC%98%E5%8C%96"><span class="nav-number">3.0.3.</span> <span class="nav-text">编解码结构优化</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%9B%9B-%E6%95%B0%E6%8D%AE%E4%BC%A0%E8%BE%93"><span class="nav-number">4.</span> <span class="nav-text">四. 数据传输</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BC%A0%E8%BE%93%E9%80%9F%E7%8E%87"><span class="nav-number">4.0.1.</span> <span class="nav-text">传输速率</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%99%8D%E4%BD%8E%E5%BB%B6%E8%BF%9F"><span class="nav-number">4.0.2.</span> <span class="nav-text">降低延迟</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%A8%B3%E5%AE%9A%E7%BD%91%E7%BB%9C"><span class="nav-number">4.0.3.</span> <span class="nav-text">稳定网络</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%BA%94-%E8%A7%86%E7%82%B9%E5%90%88%E6%88%90"><span class="nav-number">5.</span> <span class="nav-text">五. 视点合成</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#DIBR%E5%AD%98%E5%9C%A8%E7%9A%84%E9%97%AE%E9%A2%98"><span class="nav-number">5.0.1.</span> <span class="nav-text">DIBR存在的问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%8F%90%E9%AB%98%E8%87%AA%E7%94%B1%E8%A7%86%E7%82%B9%E8%A7%86%E9%A2%91%E7%9A%84%E8%B4%A8%E9%87%8F"><span class="nav-number">5.0.2.</span> <span class="nav-text">提高自由视点视频的质量</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%96%87%E7%8C%AE%E5%8F%82%E8%80%83"><span class="nav-number">6.</span> <span class="nav-text">文献参考</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="luxutytheghost"
      src="/images/aqua.jpg">
  <p class="site-author-name" itemprop="name">luxutytheghost</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">3</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">1</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/luxurytheghost" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;luxurytheghost" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">luxutytheghost</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
  </div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>


  <script defer src="/lib/three/three.min.js"></script>
    <script defer src="/lib/three/three-waves.min.js"></script>


  




  
<script src="/js/local-search.js"></script>













  

  

<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"model":{"jsonPath":"/live2dw/assets/shizuku.model.json"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":true},"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>
</html>
